{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiements with NER on MS MARCO Document dataset\n",
    "\n",
    "This notebook is a sandbox for using ðŸ¤— Transformers for NER in the indexing pipeline and at search time for MS MARCO Document ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project library\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import qopt\n",
    "importlib.reload(qopt)\n",
    "\n",
    "from qopt.trec import load_queries_as_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'dbmdz/bert-large-cased-finetuned-conll03-english'\n",
    "# model_name = 'dbmdz/bert-base-cased-finetuned-conll03-english'\n",
    "# model_name = 'sshleifer/tiny-dbmdz-bert-large-cased-finetuned-conll03-english'\n",
    "# model_name = 'mrm8488/mobilebert-finetuned-ner'\n",
    "model_name = 'elastic/distilbert-base-cased-finetuned-conll03-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512, padding=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "ner = pipeline('ner', tokenizer=tokenizer, model=model, grouped_entities=True, ignore_subwords=True)\n",
    "ner_ungrouped = pipeline('ner', tokenizer=tokenizer, model=model, grouped_entities=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner([\n",
    "#     \"This is a sentence by John Smith\",\n",
    "#     \"I smell chocolate maybe a Mars bar or something made by Nestle in Switzerland\",\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 260.832555\n",
      "Size (MB): 133.440079\n"
     ]
    }
   ],
   "source": [
    "# From:\n",
    "#  - https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html\n",
    "#  - https://pytorch.org/docs/stable/quantization.html#dynamic-quantization\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "model.to(\"cpu\")\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "print_size_of_model(model)\n",
    "print_size_of_model(quantized_model)\n",
    "\n",
    "quantized_ner = pipeline('ner', tokenizer=tokenizer, model=quantized_model, grouped_entities=True, ignore_subwords=True)\n",
    "quantized_ner_ungrouped = pipeline('ner', tokenizer=tokenizer, model=quantized_model, grouped_entities=False, ignore_labels=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 119 ms, sys: 6.76 ms, total: 126 ms\n",
      "Wall time: 127 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.9840184450149536,\n",
       "  'word': 'Elasticsearch',\n",
       "  'start': 0,\n",
       "  'end': 13},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.968928337097168,\n",
       "  'word': 'Kibana',\n",
       "  'start': 18,\n",
       "  'end': 24},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9917063117027283,\n",
       "  'word': 'Elastic',\n",
       "  'start': 43,\n",
       "  'end': 50},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9993715286254883,\n",
       "  'word': 'Amsterdam',\n",
       "  'start': 69,\n",
       "  'end': 78},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.7701449394226074,\n",
       "  'word': 'Elastic',\n",
       "  'start': 104,\n",
       "  'end': 111},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9989482462406158,\n",
       "  'word': 'US Bank',\n",
       "  'start': 122,\n",
       "  'end': 129},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.9997017085552216,\n",
       "  'word': 'John Smith',\n",
       "  'start': 201,\n",
       "  'end': 211},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9980860352516174,\n",
       "  'word': 'Accenture',\n",
       "  'start': 236,\n",
       "  'end': 245},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.9989999532699585,\n",
       "  'word': 'Max Musterman',\n",
       "  'start': 263,\n",
       "  'end': 276},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.999583512544632,\n",
       "  'word': 'US Bank',\n",
       "  'start': 281,\n",
       "  'end': 288}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ner(\"Elasticsearch and Kibana are products from Elastic which is based in Amsterdam. In the context of using Elasticsearch for US Bank, we see similarities with other observability use-cases. We spoke with John Smith and his collegaues from Accenture who confirmed to Max Mustermann at US Bank that their use-case would fit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 109 ms, sys: 5.51 ms, total: 114 ms\n",
      "Wall time: 52.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'El',\n",
       "  'score': 0.5598241090774536,\n",
       "  'entity': 'O',\n",
       "  'index': 1,\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'word': '##astic',\n",
       "  'score': 0.6717578768730164,\n",
       "  'entity': 'O',\n",
       "  'index': 2,\n",
       "  'start': 2,\n",
       "  'end': 7},\n",
       " {'word': '##sea',\n",
       "  'score': 0.3697337210178375,\n",
       "  'entity': 'I-MISC',\n",
       "  'index': 3,\n",
       "  'start': 7,\n",
       "  'end': 10},\n",
       " {'word': '##rch',\n",
       "  'score': 0.7620847821235657,\n",
       "  'entity': 'O',\n",
       "  'index': 4,\n",
       "  'start': 10,\n",
       "  'end': 13},\n",
       " {'word': 'and',\n",
       "  'score': 0.9988463521003723,\n",
       "  'entity': 'O',\n",
       "  'index': 5,\n",
       "  'start': 14,\n",
       "  'end': 17},\n",
       " {'word': 'Ki',\n",
       "  'score': 0.5805865526199341,\n",
       "  'entity': 'B-ORG',\n",
       "  'index': 6,\n",
       "  'start': 18,\n",
       "  'end': 20},\n",
       " {'word': '##bana',\n",
       "  'score': 0.8712792992591858,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 7,\n",
       "  'start': 20,\n",
       "  'end': 24},\n",
       " {'word': 'are',\n",
       "  'score': 0.9986699819564819,\n",
       "  'entity': 'O',\n",
       "  'index': 8,\n",
       "  'start': 25,\n",
       "  'end': 28},\n",
       " {'word': 'products',\n",
       "  'score': 0.9995617270469666,\n",
       "  'entity': 'O',\n",
       "  'index': 9,\n",
       "  'start': 29,\n",
       "  'end': 37},\n",
       " {'word': 'from',\n",
       "  'score': 0.9985780715942383,\n",
       "  'entity': 'O',\n",
       "  'index': 10,\n",
       "  'start': 38,\n",
       "  'end': 42},\n",
       " {'word': 'El',\n",
       "  'score': 0.8589796423912048,\n",
       "  'entity': 'B-ORG',\n",
       "  'index': 11,\n",
       "  'start': 43,\n",
       "  'end': 45},\n",
       " {'word': '##astic',\n",
       "  'score': 0.4980684816837311,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 12,\n",
       "  'start': 45,\n",
       "  'end': 50},\n",
       " {'word': 'which',\n",
       "  'score': 0.9940417408943176,\n",
       "  'entity': 'O',\n",
       "  'index': 13,\n",
       "  'start': 51,\n",
       "  'end': 56},\n",
       " {'word': 'is',\n",
       "  'score': 0.9993036389350891,\n",
       "  'entity': 'O',\n",
       "  'index': 14,\n",
       "  'start': 57,\n",
       "  'end': 59},\n",
       " {'word': 'based',\n",
       "  'score': 0.9991726875305176,\n",
       "  'entity': 'O',\n",
       "  'index': 15,\n",
       "  'start': 60,\n",
       "  'end': 65},\n",
       " {'word': 'in',\n",
       "  'score': 0.9990749359130859,\n",
       "  'entity': 'O',\n",
       "  'index': 16,\n",
       "  'start': 66,\n",
       "  'end': 68},\n",
       " {'word': 'Amsterdam',\n",
       "  'score': 0.9893618822097778,\n",
       "  'entity': 'B-LOC',\n",
       "  'index': 17,\n",
       "  'start': 69,\n",
       "  'end': 78},\n",
       " {'word': '.',\n",
       "  'score': 0.999579906463623,\n",
       "  'entity': 'O',\n",
       "  'index': 18,\n",
       "  'start': 78,\n",
       "  'end': 79},\n",
       " {'word': 'In',\n",
       "  'score': 0.9997059106826782,\n",
       "  'entity': 'O',\n",
       "  'index': 19,\n",
       "  'start': 80,\n",
       "  'end': 82},\n",
       " {'word': 'the',\n",
       "  'score': 0.9999207258224487,\n",
       "  'entity': 'O',\n",
       "  'index': 20,\n",
       "  'start': 83,\n",
       "  'end': 86},\n",
       " {'word': 'context',\n",
       "  'score': 0.9997233748435974,\n",
       "  'entity': 'O',\n",
       "  'index': 21,\n",
       "  'start': 87,\n",
       "  'end': 94},\n",
       " {'word': 'of',\n",
       "  'score': 0.9998945593833923,\n",
       "  'entity': 'O',\n",
       "  'index': 22,\n",
       "  'start': 95,\n",
       "  'end': 97},\n",
       " {'word': 'using',\n",
       "  'score': 0.9997352361679077,\n",
       "  'entity': 'O',\n",
       "  'index': 23,\n",
       "  'start': 98,\n",
       "  'end': 103},\n",
       " {'word': 'El',\n",
       "  'score': 0.3766401708126068,\n",
       "  'entity': 'B-ORG',\n",
       "  'index': 24,\n",
       "  'start': 104,\n",
       "  'end': 106},\n",
       " {'word': '##astic',\n",
       "  'score': 0.342804878950119,\n",
       "  'entity': 'I-MISC',\n",
       "  'index': 25,\n",
       "  'start': 106,\n",
       "  'end': 111},\n",
       " {'word': '##sea',\n",
       "  'score': 0.3793541491031647,\n",
       "  'entity': 'O',\n",
       "  'index': 26,\n",
       "  'start': 111,\n",
       "  'end': 114},\n",
       " {'word': '##rch',\n",
       "  'score': 0.8748388290405273,\n",
       "  'entity': 'O',\n",
       "  'index': 27,\n",
       "  'start': 114,\n",
       "  'end': 117},\n",
       " {'word': 'for',\n",
       "  'score': 0.999617874622345,\n",
       "  'entity': 'O',\n",
       "  'index': 28,\n",
       "  'start': 118,\n",
       "  'end': 121},\n",
       " {'word': 'US',\n",
       "  'score': 0.9912194609642029,\n",
       "  'entity': 'B-ORG',\n",
       "  'index': 29,\n",
       "  'start': 122,\n",
       "  'end': 124},\n",
       " {'word': 'Bank',\n",
       "  'score': 0.9793050289154053,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 30,\n",
       "  'start': 125,\n",
       "  'end': 129},\n",
       " {'word': ',',\n",
       "  'score': 0.9999061822891235,\n",
       "  'entity': 'O',\n",
       "  'index': 31,\n",
       "  'start': 129,\n",
       "  'end': 130},\n",
       " {'word': 'we',\n",
       "  'score': 0.9994423985481262,\n",
       "  'entity': 'O',\n",
       "  'index': 32,\n",
       "  'start': 131,\n",
       "  'end': 133},\n",
       " {'word': 'see',\n",
       "  'score': 0.9997963309288025,\n",
       "  'entity': 'O',\n",
       "  'index': 33,\n",
       "  'start': 134,\n",
       "  'end': 137},\n",
       " {'word': 'similarities',\n",
       "  'score': 0.9997726678848267,\n",
       "  'entity': 'O',\n",
       "  'index': 34,\n",
       "  'start': 138,\n",
       "  'end': 150},\n",
       " {'word': 'with',\n",
       "  'score': 0.9999141097068787,\n",
       "  'entity': 'O',\n",
       "  'index': 35,\n",
       "  'start': 151,\n",
       "  'end': 155},\n",
       " {'word': 'other',\n",
       "  'score': 0.9999228119850159,\n",
       "  'entity': 'O',\n",
       "  'index': 36,\n",
       "  'start': 156,\n",
       "  'end': 161},\n",
       " {'word': 'o',\n",
       "  'score': 0.999906599521637,\n",
       "  'entity': 'O',\n",
       "  'index': 37,\n",
       "  'start': 162,\n",
       "  'end': 163},\n",
       " {'word': '##bs',\n",
       "  'score': 0.9962908029556274,\n",
       "  'entity': 'O',\n",
       "  'index': 38,\n",
       "  'start': 163,\n",
       "  'end': 165},\n",
       " {'word': '##er',\n",
       "  'score': 0.9975517988204956,\n",
       "  'entity': 'O',\n",
       "  'index': 39,\n",
       "  'start': 165,\n",
       "  'end': 167},\n",
       " {'word': '##va',\n",
       "  'score': 0.9991315603256226,\n",
       "  'entity': 'O',\n",
       "  'index': 40,\n",
       "  'start': 167,\n",
       "  'end': 169},\n",
       " {'word': '##bility',\n",
       "  'score': 0.9994180202484131,\n",
       "  'entity': 'O',\n",
       "  'index': 41,\n",
       "  'start': 169,\n",
       "  'end': 175},\n",
       " {'word': 'use',\n",
       "  'score': 0.9983628392219543,\n",
       "  'entity': 'O',\n",
       "  'index': 42,\n",
       "  'start': 176,\n",
       "  'end': 179},\n",
       " {'word': '-',\n",
       "  'score': 0.9995561242103577,\n",
       "  'entity': 'O',\n",
       "  'index': 43,\n",
       "  'start': 179,\n",
       "  'end': 180},\n",
       " {'word': 'cases',\n",
       "  'score': 0.999657392501831,\n",
       "  'entity': 'O',\n",
       "  'index': 44,\n",
       "  'start': 180,\n",
       "  'end': 185},\n",
       " {'word': '.',\n",
       "  'score': 0.9998084902763367,\n",
       "  'entity': 'O',\n",
       "  'index': 45,\n",
       "  'start': 185,\n",
       "  'end': 186},\n",
       " {'word': 'We',\n",
       "  'score': 0.994621217250824,\n",
       "  'entity': 'O',\n",
       "  'index': 46,\n",
       "  'start': 187,\n",
       "  'end': 189},\n",
       " {'word': 'spoke',\n",
       "  'score': 0.9996160268783569,\n",
       "  'entity': 'O',\n",
       "  'index': 47,\n",
       "  'start': 190,\n",
       "  'end': 195},\n",
       " {'word': 'with',\n",
       "  'score': 0.9997605085372925,\n",
       "  'entity': 'O',\n",
       "  'index': 48,\n",
       "  'start': 196,\n",
       "  'end': 200},\n",
       " {'word': 'John',\n",
       "  'score': 0.998145341873169,\n",
       "  'entity': 'B-PER',\n",
       "  'index': 49,\n",
       "  'start': 201,\n",
       "  'end': 205},\n",
       " {'word': 'Smith',\n",
       "  'score': 0.9992055892944336,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 50,\n",
       "  'start': 206,\n",
       "  'end': 211},\n",
       " {'word': 'and',\n",
       "  'score': 0.9998036623001099,\n",
       "  'entity': 'O',\n",
       "  'index': 51,\n",
       "  'start': 212,\n",
       "  'end': 215},\n",
       " {'word': 'his',\n",
       "  'score': 0.999818742275238,\n",
       "  'entity': 'O',\n",
       "  'index': 52,\n",
       "  'start': 216,\n",
       "  'end': 219},\n",
       " {'word': 'co',\n",
       "  'score': 0.9982821941375732,\n",
       "  'entity': 'O',\n",
       "  'index': 53,\n",
       "  'start': 220,\n",
       "  'end': 222},\n",
       " {'word': '##lle',\n",
       "  'score': 0.46656376123428345,\n",
       "  'entity': 'O',\n",
       "  'index': 54,\n",
       "  'start': 222,\n",
       "  'end': 225},\n",
       " {'word': '##gau',\n",
       "  'score': 0.5542685389518738,\n",
       "  'entity': 'O',\n",
       "  'index': 55,\n",
       "  'start': 225,\n",
       "  'end': 228},\n",
       " {'word': '##es',\n",
       "  'score': 0.9905117154121399,\n",
       "  'entity': 'O',\n",
       "  'index': 56,\n",
       "  'start': 228,\n",
       "  'end': 230},\n",
       " {'word': 'from',\n",
       "  'score': 0.8753461837768555,\n",
       "  'entity': 'O',\n",
       "  'index': 57,\n",
       "  'start': 231,\n",
       "  'end': 235},\n",
       " {'word': 'A',\n",
       "  'score': 0.6226997375488281,\n",
       "  'entity': 'B-ORG',\n",
       "  'index': 58,\n",
       "  'start': 236,\n",
       "  'end': 237},\n",
       " {'word': '##cc',\n",
       "  'score': 0.5866755843162537,\n",
       "  'entity': 'O',\n",
       "  'index': 59,\n",
       "  'start': 237,\n",
       "  'end': 239},\n",
       " {'word': '##ent',\n",
       "  'score': 0.6985529065132141,\n",
       "  'entity': 'O',\n",
       "  'index': 60,\n",
       "  'start': 239,\n",
       "  'end': 242},\n",
       " {'word': '##ure',\n",
       "  'score': 0.606274425983429,\n",
       "  'entity': 'O',\n",
       "  'index': 61,\n",
       "  'start': 242,\n",
       "  'end': 245},\n",
       " {'word': 'who',\n",
       "  'score': 0.9988327026367188,\n",
       "  'entity': 'O',\n",
       "  'index': 62,\n",
       "  'start': 246,\n",
       "  'end': 249},\n",
       " {'word': 'confirmed',\n",
       "  'score': 0.9988510012626648,\n",
       "  'entity': 'O',\n",
       "  'index': 63,\n",
       "  'start': 250,\n",
       "  'end': 259},\n",
       " {'word': 'to',\n",
       "  'score': 0.9989225268363953,\n",
       "  'entity': 'O',\n",
       "  'index': 64,\n",
       "  'start': 260,\n",
       "  'end': 262},\n",
       " {'word': 'Max',\n",
       "  'score': 0.983818769454956,\n",
       "  'entity': 'B-PER',\n",
       "  'index': 65,\n",
       "  'start': 263,\n",
       "  'end': 266},\n",
       " {'word': 'Must',\n",
       "  'score': 0.9880390763282776,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 66,\n",
       "  'start': 267,\n",
       "  'end': 271},\n",
       " {'word': '##erman',\n",
       "  'score': 0.5665251612663269,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 67,\n",
       "  'start': 271,\n",
       "  'end': 276},\n",
       " {'word': '##n',\n",
       "  'score': 0.6465468406677246,\n",
       "  'entity': 'O',\n",
       "  'index': 68,\n",
       "  'start': 276,\n",
       "  'end': 277},\n",
       " {'word': 'at',\n",
       "  'score': 0.9929287433624268,\n",
       "  'entity': 'O',\n",
       "  'index': 69,\n",
       "  'start': 278,\n",
       "  'end': 280},\n",
       " {'word': 'US',\n",
       "  'score': 0.9978768229484558,\n",
       "  'entity': 'B-ORG',\n",
       "  'index': 70,\n",
       "  'start': 281,\n",
       "  'end': 283},\n",
       " {'word': 'Bank',\n",
       "  'score': 0.9978262782096863,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 71,\n",
       "  'start': 284,\n",
       "  'end': 288},\n",
       " {'word': 'that',\n",
       "  'score': 0.99980229139328,\n",
       "  'entity': 'O',\n",
       "  'index': 72,\n",
       "  'start': 289,\n",
       "  'end': 293},\n",
       " {'word': 'their',\n",
       "  'score': 0.9997331500053406,\n",
       "  'entity': 'O',\n",
       "  'index': 73,\n",
       "  'start': 294,\n",
       "  'end': 299},\n",
       " {'word': 'use',\n",
       "  'score': 0.9972354769706726,\n",
       "  'entity': 'O',\n",
       "  'index': 74,\n",
       "  'start': 300,\n",
       "  'end': 303},\n",
       " {'word': '-',\n",
       "  'score': 0.9979692697525024,\n",
       "  'entity': 'O',\n",
       "  'index': 75,\n",
       "  'start': 303,\n",
       "  'end': 304},\n",
       " {'word': 'case',\n",
       "  'score': 0.9969545006752014,\n",
       "  'entity': 'O',\n",
       "  'index': 76,\n",
       "  'start': 304,\n",
       "  'end': 308},\n",
       " {'word': 'would',\n",
       "  'score': 0.9996899366378784,\n",
       "  'entity': 'O',\n",
       "  'index': 77,\n",
       "  'start': 309,\n",
       "  'end': 314},\n",
       " {'word': 'fit',\n",
       "  'score': 0.9992860555648804,\n",
       "  'entity': 'O',\n",
       "  'index': 78,\n",
       "  'start': 315,\n",
       "  'end': 318},\n",
       " {'word': '.',\n",
       "  'score': 0.9996861219406128,\n",
       "  'entity': 'O',\n",
       "  'index': 79,\n",
       "  'start': 318,\n",
       "  'end': 319}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "quantized_ner_ungrouped(\"Elasticsearch and Kibana are products from Elastic which is based in Amsterdam. In the context of using Elasticsearch for US Bank, we see similarities with other observability use-cases. We spoke with John Smith and his collegaues from Accenture who confirmed to Max Mustermann at US Bank that their use-case would fit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ner(\"Elasticsearch and Kibana are products from Elastic which is based in Amsterdam.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ner(\"What were the key parts of the Manhattan Project? Does it take place in New York or is that just something that people read about on Wikipedia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ner_ungrouped(\"Elasticsearch and Kibana are products from Elastic which is based in Amsterdam. In the context of using Elasticsearch for US Bank, we see similarities with other observability use-cases. We spoke with John Smith and his collegaues from Accenture who confirmed to Max Mustermann at US Bank that their use-case would fit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "quantized_ner_ungrouped(\"Elasticsearch and Kibana are products from Elastic which is based in Amsterdam. In the context of using Elasticsearch for US Bank, we see similarities with other observability use-cases. We spoke with John Smith and his collegaues from Accenture who confirmed to Max Mustermann at US Bank that their use-case would fit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger as FlairSequenceTagger\n",
    "from flair.data import Sentence as FlairSentence\n",
    "from syntok import segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = FlairSequenceTagger.load('ner-fast')\n",
    "\n",
    "def flair_extract(text):\n",
    "    paragraphs = segmenter.process(text)\n",
    "\n",
    "    words = set()\n",
    "    for sentences in paragraphs:\n",
    "        for tokens in sentences:\n",
    "            flair_sentence = FlairSentence([token.value for token in tokens], use_tokenizer=False)\n",
    "            tagger.predict(flair_sentence)\n",
    "            for entity in flair_sentence.get_spans('ner'):\n",
    "                words.add(entity.text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "flair_extract(\"In the context of using Elasticsearch for US Bank, we see similarities with other observability use-cases. We spoke with John Smith and his collegaues from Accenture who confirmed to Max Mustermann at US Bank that their use-case would fit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "flair_extract(\"Elasticsearch and Kibana are products from Elastic which is based in Amsterdam.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b0f7d1df774fe6a932f3347ef39c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=1257, style=ProgressStyle(description_widthâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d450cfc164541b7863950e4f66e5f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18c980aa5144b00b2ccfec0dd9ad3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=112, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74225d8a7214793a53c85be9e812319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=39, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8367072df7d34983a2d4f67f1df37b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=438006919, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'dslim/bert-base-NER-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512, padding=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "ner_uncased = pipeline('ner', tokenizer=tokenizer, model=model, grouped_entities=True, ignore_subwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries = '../data/msmarco-document-sampled-queries.1000.tsv'\n",
    "dev_queries = '../data/msmarco/document/msmarco-docdev-queries.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries with entities in query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5193\n"
     ]
    }
   ],
   "source": [
    "train_query_tuples = list(load_queries_as_tuple(train_queries))\n",
    "dev_query_tuples = list(load_queries_as_tuple(dev_queries))\n",
    "print(len(dev_query_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2128\n",
      "3065\n",
      "5193\n"
     ]
    }
   ],
   "source": [
    "yes = 0\n",
    "no = 0\n",
    "for _, q in dev_query_tuples:\n",
    "    entities = [x['word'] for x in ner_uncased(q)]\n",
    "    if entities:\n",
    "#         print(f\" - {q}: {' '.join(entities)}\")\n",
    "        yes += 1\n",
    "    else:\n",
    "        no += 1\n",
    "print(yes)\n",
    "print(no)\n",
    "print(yes+no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 39% of the 10,000 train set have entities ~= 3,900\n",
    "- 41% of the 1,000 train set have entities ~= 418\n",
    "- 41% of the 5,200 dev set have entities ~= 2,128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries with entities in the top-result document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers.file_utils import add_end_docstrings, is_tf_available, is_torch_available\n",
    "from transformers.modelcard import ModelCard\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\n",
    "from transformers.models.bert.tokenization_bert import BasicTokenizer\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.pipelines import *\n",
    "from transformers.pipelines.base import PIPELINE_INIT_ARGS, ArgumentHandler, Pipeline\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from typing import TYPE_CHECKING, List, Optional, Union\n",
    "\n",
    "class MyTokenClassificationPipeline(Pipeline):\n",
    "    default_input_names = \"sequences\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        modelcard: Optional[ModelCard] = None,\n",
    "        framework: Optional[str] = None,\n",
    "        args_parser: ArgumentHandler = TokenClassificationArgumentHandler(),\n",
    "        device: int = -1,\n",
    "        binary_output: bool = False,\n",
    "        ignore_labels=[\"O\"],\n",
    "        task: str = \"\",\n",
    "        grouped_entities: bool = False,\n",
    "        ignore_subwords: bool = False,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            modelcard=modelcard,\n",
    "            framework=framework,\n",
    "            device=device,\n",
    "            binary_output=binary_output,\n",
    "            task=task,\n",
    "        )\n",
    "\n",
    "        self.check_model_type(MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING)\n",
    "\n",
    "        self._basic_tokenizer = BasicTokenizer(do_lower_case=False)\n",
    "        self._args_parser = args_parser\n",
    "        self.ignore_labels = ignore_labels\n",
    "        self.grouped_entities = grouped_entities\n",
    "        self.ignore_subwords = ignore_subwords\n",
    "\n",
    "        if self.ignore_subwords and not self.tokenizer.is_fast:\n",
    "            raise ValueError(\n",
    "                \"Slow tokenizers cannot ignore subwords. Please set the `ignore_subwords` option\"\n",
    "                \"to `False` or use a fast tokenizer.\"\n",
    "            )\n",
    "\n",
    "    def __call__(self, inputs: Union[str, List[str]], **kwargs):\n",
    "        _inputs, offset_mappings = self._args_parser(inputs, **kwargs)\n",
    "\n",
    "        answers = []\n",
    "\n",
    "        for i, sentence in enumerate(_inputs):\n",
    "\n",
    "            # Manage correct placement of the tensors\n",
    "            with self.device_placement():\n",
    "\n",
    "                tokens = self.tokenizer(\n",
    "                    sentence,\n",
    "                    return_attention_mask=False,\n",
    "                    return_tensors=self.framework,\n",
    "                    truncation=True,\n",
    "                    return_special_tokens_mask=True,\n",
    "                    return_offsets_mapping=self.tokenizer.is_fast,\n",
    "                )\n",
    "                if self.tokenizer.is_fast:\n",
    "                    offset_mapping = tokens.pop(\"offset_mapping\").cpu().numpy()[0]\n",
    "                elif offset_mappings:\n",
    "                    offset_mapping = offset_mappings[i]\n",
    "                else:\n",
    "                    offset_mapping = None\n",
    "\n",
    "                special_tokens_mask = tokens.pop(\"special_tokens_mask\").cpu().numpy()[0]\n",
    "\n",
    "                # Forward\n",
    "                if self.framework == \"tf\":\n",
    "                    entities = self.model(tokens.data)[0][0].numpy()\n",
    "                    input_ids = tokens[\"input_ids\"].numpy()[0]\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        tokens = self.ensure_tensor_on_device(**tokens)\n",
    "                        entities = self.model(**tokens)[0][0].cpu().numpy()\n",
    "                        input_ids = tokens[\"input_ids\"].cpu().numpy()[0]\n",
    "\n",
    "            score = np.exp(entities) / np.exp(entities).sum(-1, keepdims=True)\n",
    "            labels_idx = score.argmax(axis=-1)\n",
    "\n",
    "            entities = []\n",
    "            # Filter to labels not in `self.ignore_labels`\n",
    "            # Filter special_tokens\n",
    "            filtered_labels_idx = [\n",
    "                (idx, label_idx)\n",
    "                for idx, label_idx in enumerate(labels_idx)\n",
    "                if (self.model.config.id2label[label_idx] not in self.ignore_labels) and not special_tokens_mask[idx]\n",
    "            ]\n",
    "\n",
    "            for idx, label_idx in filtered_labels_idx:\n",
    "                if offset_mapping is not None:\n",
    "                    start_ind, end_ind = offset_mapping[idx]\n",
    "                    word_ref = sentence[start_ind:end_ind]\n",
    "                    word = self.tokenizer.convert_ids_to_tokens([int(input_ids[idx])])[0]\n",
    "                    is_subword = len(word_ref) != len(word)\n",
    "\n",
    "                    if int(input_ids[idx]) == self.tokenizer.unk_token_id:\n",
    "                        word = word_ref\n",
    "                        is_subword = False\n",
    "                else:\n",
    "                    word = self.tokenizer.convert_ids_to_tokens(int(input_ids[idx]))\n",
    "\n",
    "                    start_ind = None\n",
    "                    end_ind = None\n",
    "\n",
    "                entity = {\n",
    "                    \"word\": word,\n",
    "                    \"score\": score[idx][label_idx].item(),\n",
    "                    \"entity\": self.model.config.id2label[label_idx],\n",
    "                    \"index\": idx,\n",
    "                    \"start\": start_ind,\n",
    "                    \"end\": end_ind,\n",
    "                }\n",
    "\n",
    "                if self.grouped_entities and self.ignore_subwords:\n",
    "                    entity[\"is_subword\"] = is_subword\n",
    "\n",
    "                entities += [entity]\n",
    "\n",
    "            if self.grouped_entities:\n",
    "                answers += [self.group_entities(entities)]\n",
    "            # Append ungrouped entities\n",
    "            else:\n",
    "                answers += [entities]\n",
    "\n",
    "        if len(answers) == 1:\n",
    "            return answers[0]\n",
    "        return answers\n",
    "\n",
    "    def group_sub_entities(self, entities: List[dict]) -> dict:\n",
    "        \"\"\"\n",
    "        Group together the adjacent tokens with the same entity predicted.\n",
    "\n",
    "        Args:\n",
    "            entities (:obj:`dict`): The entities predicted by the pipeline.\n",
    "        \"\"\"\n",
    "        # Get the first entity in the entity group\n",
    "        entity = entities[0][\"entity\"].split(\"-\")[-1]\n",
    "        scores = np.nanmean([entity[\"score\"] for entity in entities])\n",
    "        tokens = [entity[\"word\"] for entity in entities]\n",
    "\n",
    "        entity_group = {\n",
    "            \"entity_group\": entity,\n",
    "            \"score\": np.mean(scores),\n",
    "            \"word\": self.tokenizer.convert_tokens_to_string(tokens),\n",
    "            \"start\": entities[0][\"start\"],\n",
    "            \"end\": entities[-1][\"end\"],\n",
    "        }\n",
    "        return entity_group\n",
    "\n",
    "    def group_entities(self, entities: List[dict]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Find and group together the adjacent tokens with the same entity predicted.\n",
    "\n",
    "        Args:\n",
    "            entities (:obj:`dict`): The entities predicted by the pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        entity_groups = []\n",
    "        entity_group_disagg = []\n",
    "\n",
    "        if entities:\n",
    "            last_idx = entities[-1][\"index\"]\n",
    "\n",
    "        for entity in entities:\n",
    "\n",
    "            is_last_idx = entity[\"index\"] == last_idx\n",
    "            is_subword = self.ignore_subwords and entity[\"is_subword\"]\n",
    "            if not entity_group_disagg:\n",
    "                entity_group_disagg += [entity]\n",
    "                if is_last_idx:\n",
    "                    entity_groups += [self.group_sub_entities(entity_group_disagg)]\n",
    "                continue\n",
    "\n",
    "            # If the current entity is similar and adjacent to the previous entity, append it to the disaggregated entity group\n",
    "            # The split is meant to account for the \"B\" and \"I\" suffixes\n",
    "            # Shouldn't merge if both entities are B-type\n",
    "            if (\n",
    "                (\n",
    "                    entity[\"entity\"].split(\"-\")[-1] == entity_group_disagg[-1][\"entity\"].split(\"-\")[-1]\n",
    "                    and entity[\"entity\"].split(\"-\")[0] != \"B\"\n",
    "                )\n",
    "                and entity[\"index\"] == entity_group_disagg[-1][\"index\"] + 1\n",
    "            ) or is_subword:\n",
    "                # Modify subword type to be previous_type\n",
    "                if is_subword:\n",
    "                    entity[\"entity\"] = entity_group_disagg[-1][\"entity\"].split(\"-\")[-1]\n",
    "                    entity[\"score\"] = np.nan  # set ignored scores to nan and use np.nanmean\n",
    "\n",
    "                entity_group_disagg += [entity]\n",
    "                # Group the entities at the last entity\n",
    "                if is_last_idx:\n",
    "                    entity_groups += [self.group_sub_entities(entity_group_disagg)]\n",
    "            # If the current entity is different from the previous entity, aggregate the disaggregated entity group\n",
    "            else:\n",
    "                entity_groups += [self.group_sub_entities(entity_group_disagg)]\n",
    "                entity_group_disagg = [entity]\n",
    "                # If it's the last entity, add it to the entity groups\n",
    "                if is_last_idx:\n",
    "                    entity_groups += [self.group_sub_entities(entity_group_disagg)]\n",
    "\n",
    "        return entity_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ner = MyTokenClassificationPipeline(tokenizer=tokenizer, model=model, grouped_entities=True, ignore_subwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '##astic',\n",
       "  'score': 0.8498260378837585,\n",
       "  'entity': 'I-MISC',\n",
       "  'index': 2,\n",
       "  'start': 2,\n",
       "  'end': 7},\n",
       " {'word': '##sea',\n",
       "  'score': 0.7780320644378662,\n",
       "  'entity': 'I-MISC',\n",
       "  'index': 3,\n",
       "  'start': 7,\n",
       "  'end': 10},\n",
       " {'word': 'El',\n",
       "  'score': 0.9576388597488403,\n",
       "  'entity': 'B-ORG',\n",
       "  'index': 10,\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'word': '##astic',\n",
       "  'score': 0.8840276002883911,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 11,\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'word': 'Amsterdam',\n",
       "  'score': 0.998390257358551,\n",
       "  'entity': 'B-LOC',\n",
       "  'index': 13,\n",
       "  'start': 46,\n",
       "  'end': 55}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_ungrouped(\"Elasticsearch is a thing made by Elastic from Amsterdam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.9990760087966919,\n",
       "  'word': 'Elasticsearch',\n",
       "  'start': 0,\n",
       "  'end': 13},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.996468722820282,\n",
       "  'word': 'Kibana',\n",
       "  'start': 18,\n",
       "  'end': 24},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9992861747741699,\n",
       "  'word': 'Elastic',\n",
       "  'start': 43,\n",
       "  'end': 50},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9995539784431458,\n",
       "  'word': 'Amsterdam',\n",
       "  'start': 69,\n",
       "  'end': 78}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\"Elasticsearch and Kibana are products from Elastic which is based in Amsterdam.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.file_utils import hf_bucket_url, cached_path\n",
    "pretrained_model_name = 'elastic/distilbert-base-cased-finetuned-conll03-english'\n",
    "archive_file = hf_bucket_url(\n",
    "    pretrained_model_name,\n",
    "    filename='pytorch_model.bin',\n",
    ")\n",
    "resolved_archive_file = cached_path(archive_file)\n",
    "resolved_archive_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
